{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Orientation\n",
    "\n",
    "Welcome to *Jupyter*. Through this interface, you'll be learning a lot of things:\n",
    "\n",
    "* A Programming language: **Python**\n",
    "* A Python library: **NLTK**\n",
    "* Overlapping research areas: **Corpus linguistics**, **Natural language processing**, **Distant reading**\n",
    "* Additional skills: **Regular Expressions**, some **Shell commands**, and **tips on managing your data**\n",
    "\n",
    "You can head [here](https://github.com/interrogator/wwc) for the fully articulated overview of the course, but we'll almost always stay within this **Jupyter** interface.\n",
    "\n",
    "Remember, everything we cover here will remain available to you after we're through is over, including a completed version of these notebooks. It's all accessible via [GitHub](https://github.com/interrogator/wwc).\n",
    "\n",
    "**Any questions before we begin?**\n",
    "\n",
    "Alright, we're off!\n",
    "\n",
    "## Text as data\n",
    "\n",
    "Programming languages like Python are great for processing data. In order to apply it to *text*, we need to think about our text as data. This means being aware of how text is structured, what extra information might be encoded in it, and how to manage to give the best results.\n",
    "\n",
    "## What is the Natural Language Toolkit?\n",
    "\n",
    "We'll be covering some of the theory behind corpus linguistics later on, but let's start by looking at some of the tasks NLTK can help you with.\n",
    "\n",
    "NLTK is a Python Library for working with written language data. It is free and extensively documented. Many areas we'll be covering are treated in more detail in the NLTK Book, available free online from [here](http://www.nltk.org/book/). \n",
    "\n",
    "> Note: NLTK provides tools for tasks ranging from very simple (counting words in a text) to very complex (writing and training parsers, etc.). Many advanced tasks are beyond the scope of this course, but by the time we're done, you should understand Python and NLTK well enough to perform these tasks on your own!\n",
    "\n",
    "We will start by importing NLTK, setting a path to NLTK resources, and downloading some additional stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk # Use for importing the nltk library\n",
    "\n",
    "user_nltk_dir = \"/home/researcher/nltk_data\" # Specify our data directory\n",
    "if user_nltk_dir not in nltk.data.path: # Make sure nltk can access this dir.\n",
    "    nltk.data.path.insert(0, user_nltk_dir)\n",
    "nltk.download(\"book\", download_dir=user_nltk_dir, quiet=True) # Download book materials to data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-driven programming\n",
    "\n",
    "In test-driven design, you define your expected output, and then write code that achieves that. This makes it easy to keep track of your progress, and prevents *feature creep*.\n",
    "\n",
    "For the last part of the session, explore some NLTK texts, using some of Python's *operators*, listed below.\n",
    "\n",
    "### Common relationals\n",
    "\n",
    "|  Relational | Meaning |\n",
    "|--------------:|:------------|\n",
    "| <    |  less than |\n",
    "| <=   |   less than or equal to |\n",
    "| ==  |    equal to (note this is two \"=\" signs, not one) |\n",
    "| !=   |   not equal to |\n",
    "| \\>   |   greater than |\n",
    "| \\>= |   greater than or equal to |\n",
    "\n",
    "### Common operators\n",
    "\n",
    "| Operator  | Purpose  |\n",
    "|-----------|------------|\n",
    "| s.startswith(t) | test if s starts with t |\n",
    "| s.endswith(t)  |  test if s ends with t |\n",
    "| t in s         |  test if t is a substring of s |\n",
    "| s.islower()    |  test if s contains cased characters and all are lowercase |\n",
    "| s.isupper()    |  test if s contains cased characters and all are uppercase |\n",
    "| s.isalpha()    |  test if s is non-empty and all characters in s are alphabetic |\n",
    "| s.isalnum()    |  test if s is non-empty and all characters in s are alphanumeric |\n",
    "| s.isdigit()    |  test if s is non-empty and all characters in s are digits |\n",
    "| s.istitle()    |  test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus!\n",
    "\n",
    "You'll remember right at the beginning we started looking at the size of the vocabulary of a text, but there were two problems with the results we got from using `len(set(text1))`. This count includes items of punctuation and treats capitalised and non-capitalised words as different things (*This* vs *this*). We can now fix these problems. We start by getting rid of capitalised words, then we get rid of the punctuation and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You've now had a really great peek into how computers understand written text. In the next session, we'll have all the foundations we need to do some common corpus linguistic tasks, like collocation, keywording and n-gramming."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
