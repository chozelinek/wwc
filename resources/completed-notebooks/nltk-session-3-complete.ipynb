{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Annotating data\n",
    "\n",
    "In this session, we introduce part of speech tagging, use it to do lemmatisation, and develop a function that can concordance a tagged corpus.\n",
    "\n",
    "## POS tagging\n",
    "\n",
    "Some NLTK corpora have POS tags already in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "print(brown.words())\n",
    "print(brown.tagged_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract adverbs from the Brown Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs = []\n",
    "for word, tag in brown.tagged_words():\n",
    "    # get any word whose tag is adverb\n",
    "    if tag == 'RB':\n",
    "        adverbs.append(word)\n",
    "adverbs[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice the method above as a common pattern in Python code. Declare an empty `list` or `dict`, iterate over a dataset, and add matches in there. This can turn into a lot of code though, so sometimes people use generator expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs = [word for word, tag in brown.tagged_words() if tag == 'RB'][:50]\n",
    "adverbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of these two approaches?\n",
    "\n",
    "## Tallying annotated data\n",
    "\n",
    "We can use Pandas and `Counter` to learn a little about the frequency of various tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "data = Counter()\n",
    "for word, tag in brown.tagged_words():\n",
    "    # add this condition second time round\n",
    "    if word.isalnum():\n",
    "        data[tag[:2]] += 1\n",
    "df = pd.Series(list(data.values()), index = list(data.keys()))\n",
    "df = df.sort_values(ascending = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of data can then be plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.plot(title = 'Common tags', kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it's ugly, we can start customising the style a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "df.plot(title = 'Common tags', kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we could be more specific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = Counter()\n",
    "for word, tag in brown.tagged_words():\n",
    "    if tag.startswith('V'):\n",
    "        vs[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(list(vs.values()), index = list(vs.keys()))\n",
    "df = df.sort_values(ascending = False)[:10]\n",
    "df.plot(title = 'Common verbs', kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around, and see if you can plot something cool. Head [here](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) for plotting options.\n",
    "\n",
    "## Adding POS tags\n",
    "\n",
    "We can turn out plaintext into a POS tagged corpus just like the Brown Corpus (except lower accuracy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('forum.txt', 'r', encoding = 'utf-8') as fo:\n",
    "    plain = fo.read()\n",
    "section = plain[5000:10000]\n",
    "toks = nltk.word_tokenize(section)\n",
    "tagged = nltk.pos_tag(toks)\n",
    "tagged[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How easy was that?!\n",
    "\n",
    "## Searching tagged corpora\n",
    "\n",
    "Let's play around with our tagged corpus a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, tag in tagged:\n",
    "    if word.startswith('a') and tag.startswith('V'):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could get quite a bit more powerful if we also use indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (word, tag) in enumerate(tagged):\n",
    "    if tag == 'MD':\n",
    "        print(tagged[index + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using some loops and conditions to find something cool! Suggestions:\n",
    "\n",
    "1. Get long adjectives\n",
    "2. Get a list of nouns that are only preceded by indefinite articles\n",
    "3. Get the most common subjects in clauses with a modal finite\n",
    "4. Try to find passive VPs\n",
    "5. Get tokens that may be either nouns or verbs\n",
    "\n",
    "## Lemmatisation\n",
    "\n",
    "Last week, we did some stemming, and found that it's not very accurate. Why is this? Well, the basic problem is that you need to know a word's class in order to get its base form. Sure, it might make some sense to stem *liking* to *like*. But *loving* can also be nominal, as in *She had a liking for chocolate milk*. There, the stemming to *like* might not be so appropriate.\n",
    "\n",
    "With POS tags, however, we can do lemmatisation, which is more accurate. The idea is to check dictionaries for words, and only use the heuristic approaches when the word isn't found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr=WordNetLemmatizer()\n",
    "for word, tag in tagged[:50]:\n",
    "    print(lmtzr.lemmatize(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem? Yep. The lemmatiser doesn't take POS tags. Instead, it wants either `'n'`, `'v'`, `'a'`, or `'r'`. So, we need to write a translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transtag = {'N': 'n',\n",
    "            'V': 'v',\n",
    "            'J': 'a',\n",
    "            'R': 'r'}\n",
    "\n",
    "for word, tag in tagged[:50]:\n",
    "    if tag[0] in transtag.keys():\n",
    "        newtag = transtag[tag[0]]\n",
    "        print(lmtzr.lemmatize(word, newtag))\n",
    "    else:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well, but it's a big ugly. Let's improve it a bit by using a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(word, tag):\n",
    "    \"\"\"return lemma\"\"\"\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lmtzr=WordNetLemmatizer()\n",
    "\n",
    "    transtag = {'N': 'n',\n",
    "                'V': 'v',\n",
    "                'J': 'a',\n",
    "                'R': 'r'}\n",
    "    \n",
    "    if tag[0] in transtag.keys():\n",
    "        newtag = transtag[tag[0]]\n",
    "        return lmtzr.lemmatize(word, newtag)\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do something much nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, tag in tagged[:50]:\n",
    "    print(lemma(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you add the lemmatiser functionality to your old code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (word, tag) in enumerate(tagged):\n",
    "    if tag.startswith('J'):\n",
    "        nword = tagged[index + 1][0]\n",
    "        ntag = tagged[index + 1][1]\n",
    "        print(lemma(nword, ntag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a whole lot of punctuation here, which maybe isn't so helpful. How might we exlcude it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = [(word, tag) for word, tag in tagged if word.isalnum()]\n",
    "for index, (word, tag) in enumerate(corp):\n",
    "    if tag.startswith('J'):\n",
    "        nword = corp[index + 1][0]\n",
    "        ntag = corp[index + 1][1]\n",
    "        print(lemma(nword, ntag))\n",
    "\n",
    "## Return to concordancing\n",
    "\n",
    "The final thing to cover in this session is concordancing. We already saw how NLTK's concordancer works:\n",
    "\n",
    "```python\n",
    "from nltk.book import *\n",
    "text5.concordance('seriously')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but we're also aware of its limitations. Concordancing can be very powerful, especially for thematic categorisation and the like. So, let's write up a concordancer for our plain text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conc(corpus, query):\n",
    "    \"\"\"regex concordancer\"\"\"\n",
    "    import re\n",
    "    compiled = re.compile(r'(.*)(%s)(.*)' % query)\n",
    "    lines = re.findall(compiled, text)\n",
    "    for start, middle, end in lines:\n",
    "        concline = [start[-30:], middle, end[:30]]\n",
    "        print(\"\\t\".join(concline).expandtabs(35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc('austral[a-z]+', plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add a `window` keyword argument, and also fix any left printing issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conc(corpus, query, window = 30):\n",
    "    \"\"\"regex concordancer\"\"\"\n",
    "    import re\n",
    "    compiled = re.compile(r'(.*)(%s)(.*)' % query)\n",
    "    lines = re.findall(compiled, text)\n",
    "    for start, middle, end in lines:\n",
    "        concline = [start[-window:], middle, end[:window]]\n",
    "        if len(concline[0]) < window:\n",
    "            concline[0] = ' ' * (window - len(concline[0])) + concline[0]\n",
    "        print(\"\\t\".join(concline).expandtabs(35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you learn anything from our corpus by concordancing some important tokens?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
