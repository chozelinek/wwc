{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions\n",
    "\n",
    "This session goes into detail about the syntax and use-cases for regular expressions, both for searching text and working with data files and directories.\n",
    "\n",
    "## Introduction to the syntax\n",
    "\n",
    "Let's practice in [a richer environment](https://regex101.com/#python) to start with. **Turn on the 'global' flag.**\n",
    "\n",
    "As a sample text, we can use [The Love Song of J. Alfred Prufrock](http://www.bartleby.com/198/1.html), by T. S. Eliot, or perhaps [a recent open letter](http://english.khamenei.ir/news/2681/Today-terrorism-is-our-common-worry) from Sayyid Ali Khamenei to the \"youth of the West\".\n",
    "\n",
    "Then, using [Python's regex documentation](https://docs.python.org/2/library/re.html) as reference material, try to get:\n",
    "\n",
    "1. The first word of every line\n",
    "2. The last word of every line\n",
    "3. The last word of every line, if it starts with 's'\n",
    "4. Question marks\n",
    "5. Interrogatives\n",
    "6. Words beginning with capital letter that aren't line-initial\n",
    "7. All punctuation\n",
    "8. Plural nouns\n",
    "9. Text within quotation marks/brackets\n",
    "10. Hyphenates\n",
    "\n",
    "\n",
    "## The `re` module\n",
    "\n",
    "We need to import regular expression functionality before we can do anything else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.search('pattern', 'string containing the pattern')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember NLTK's concordancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "text5.concordance('seriously')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordancing can be very powerful, especially for thematic categorisation and the like. So, let's load up some data, and then write up a regex-based concordancer for our plain text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('forum.txt', 'r', encoding = 'utf-8') as fo:\n",
    "    data = fo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conc(query, corpus):\n",
    "    \"\"\"regex concordancer\"\"\"\n",
    "    import re\n",
    "    compiled = re.compile(r'(.*)(%s)(.*)' % query)\n",
    "    lines = re.findall(compiled, corpus)\n",
    "    for start, middle, end in lines:\n",
    "        concline = [start[-30:], middle, end[:30]]\n",
    "        print(\"\\t\".join(concline).expandtabs(35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conc('austral[a-z]+', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add a `window` keyword argument, and also fix any left printing issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conc(query, corpus, window = 30, n = 50):\n",
    "    \"\"\"regex concordancer\"\"\"\n",
    "    import re\n",
    "    compiled = re.compile(r'(.*)(%s)(.*)' % query, re.I)\n",
    "    lines = re.findall(compiled, corpus)\n",
    "    for start, middle, end in lines[:n]:\n",
    "        concline = [start[-window:], middle, end[:window]]\n",
    "        if len(concline[0]) < window:\n",
    "            concline[0] = ' ' * (window - len(concline[0])) + concline[0]\n",
    "        print(\"\\t\".join(concline).expandtabs(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conc(r'\\bdead[a-z-]*', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've improved on NLTK's concordancer!\n",
    "\n",
    "Can you use `conc()` to find:\n",
    "\n",
    "1. Words with exclamation marks immediately after them\n",
    "2. Multiple punctuation marks in a row\n",
    "2. tokens/words with more than three of the same vowel in a row\n",
    "3. ly adverbs with one token context either size\n",
    "5. Numbers larger than 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conc(r'[a-z]+!', data)\n",
    "conc(r'[\\!\\?,\\.]{2,}', data)\n",
    "conc(r'[a-z]*[aeiou]{4,}[a-z]*', data)\n",
    "conc(r'[^\\s]+\\s[a-z]+ly\\s[^\\s]+', data)\n",
    "conc(r'[0-9]*[123456789][0-9]+', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `re.compile()`\n",
    "\n",
    "When using regular expressions, we may want to compile our regular expression before using it. The first reason for doing this is to make things faster: compilation can take a moment, and it can slow down loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 'cat'\n",
    "print type(s)\n",
    "pattern = re.compile(s)\n",
    "print type(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flags\n",
    "\n",
    "The second reason we might want to use `re.compile()` is that we can easily add in options that change how our regular expression will match things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore case\n",
    "pattern = re.compile(s, re.I)\n",
    "# match over multiple lines\n",
    "pattern = re.compile(s, re.MULTILINE)\n",
    "# dot matches newline too\n",
    "pattern = re.compile(s, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'iraq.*\\n.*middle east.*', re.MULTILINE)\n",
    "re.findall(pattern, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `re.search()`\n",
    "\n",
    "At its simplest, you can use `re.search()` to check if a regex matches some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "found = False\n",
    "if re.search(r'p.w.r', data):\n",
    "    found = True\n",
    "    print(found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `re.findall()`\n",
    "\n",
    "We could find all numbers in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = re.findall(r'[0-9\\.]+', data)\n",
    "set(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops. Let's remove dots from the end of matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set([m.rstrip('.') for m in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can you write a regex that gives us the same output as the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in re.findall(r'[a-z]+ous\\b', data):\n",
    "    print('You are a very %s person.' % m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match = re.search(r'fe[a-z]+', data)\n",
    "type(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our pattern matches, we are left with a *match object*, which has special attributes.\n",
    "\n",
    "#### Getting character indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(match.start(), match.end())\n",
    "# or ...\n",
    "print(match.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### access the data we passed in\n",
    "print(match.string[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groups\n",
    "\n",
    "Bracketted parts of regular expressions are *groups*, which we can access using `match.group(n)`. `match.group(0)` matches the entire match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(aus)([a-z]+)', re.I)\n",
    "match = re.search(pattern, data)\n",
    "print(match.groups())\n",
    "print(match.group(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `re.match()`\n",
    "\n",
    "`re.match()` is just like `re.search()`, but it only matches start of lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.match(r'open', 'the opening of a text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.search(r'open', 'the opening of a text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, it's probably better to use `re.search()` with a caret (`^`) if need be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.search(r'^open', 'the opening of a text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `re.split()`\n",
    "\n",
    "We can create a list from a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[\\s\\.,]')\n",
    "lst = re.split(pattern, data, maxsplit = 10)\n",
    "lst[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenising with regular expressions\n",
    "\n",
    "NLTK provides [a very simply method](http://www.nltk.org/_modules/nltk/tokenize/regexp.html) for tokenising by regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# @instructor: leave out the plus and see what happens\n",
    "pattern = r'[A-Za-z0-9-]+'\n",
    "tokeniser = RegexpTokenizer(pattern)\n",
    "toks = tokeniser.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toks[100:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really, it's no different from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toks = [t for t in re.findall(pattern, data) if t]\n",
    "toks[100:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... except that has options for matching gaps rather than tokens, discarding unmatched space, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## @instructor: modify the earlier cell for this code\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "pattern = r'\\s+'\n",
    "tokeniser = RegexpTokenizer(pattern, gaps = True, discard_empty = False)\n",
    "toks = tokeniser.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `re.sub()`\n",
    "\n",
    "`re.sub()` does find and replace with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.count('moslem'))\n",
    "print(data.count('muslim'))\n",
    "fx = re.sub(r'moslems*', 'muslim', data)\n",
    "print(fx.count('muslim'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We've got the script to the [South Park: Bigger, Longer & Uncut](https://en.wikipedia.org/wiki/South_Park:_Bigger,_Longer_%26_Uncut). This film was briefly quite famous for its profanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('southparkmovie.txt', 'r', encoding = 'utf-8') as fo:\n",
    "    sp = fo.read()\n",
    "sp.count('shit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that uses regex to replace obscenities with asterisks. Bonus points for:\n",
    "\n",
    "1. Making it easy to add new obscenities to the list\n",
    "2. Printing examples as the function runs to show us that it's working\n",
    "3. Asterisks being the same length as the obscenity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def censor(plaintext):\n",
    "    \"\"\"censor naughty words\"\"\"\n",
    "    import re\n",
    "    naughty = [r'(bull)shite?', 'bloody', 'fuck', 'hell']\n",
    "    pattern = r'\\b(' + '|'.join(naughty) + r')[a-z]*'\n",
    "    regex = re.compile(pattern, re.I)\n",
    "    replaced = re.sub(regex, '****', plaintext, count=False)\n",
    "    ast = re.compile(r'\\*+')\n",
    "    for line in replaced.splitlines():\n",
    "        if re.search(ast, line):\n",
    "            print(line)\n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def censor(plaintext):\n",
    "    \"\"\"second attempt\"\"\"\n",
    "    import re\n",
    "    from nltk import word_tokenize\n",
    "    toks = word_tokenize(plaintext)\n",
    "    naughty = [r'(bull)shite?', 'bloody', 'fuck', 'hell']\n",
    "    pattern = r'\\b(' + '|'.join(naughty) + r')[a-z]*'\n",
    "    regex = re.compile(pattern, re.I)\n",
    "    for index, tok in enumerate(toks):\n",
    "        mtch = re.match(regex, tok)\n",
    "        if mtch:\n",
    "            toks[index] = '*' * len(mtch.group(0))\n",
    "            print(' '.join(toks[index - 5:index+5]))\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def censor(plaintext):\n",
    "    import re\n",
    "    naughty = [r'(bull)shite?', 'bloody', 'fuck', 'hell']\n",
    "    pattern = r'\\b(' + '|'.join(naughty) + r')[a-z]*'\n",
    "    regex = re.compile(pattern, re.I)\n",
    "    lst_of_chars = list(plaintext)\n",
    "    lines = plaintext.splitlines()\n",
    "    for index, line in enumerate(lines):\n",
    "        mtch = re.search(regex, line)\n",
    "        if mtch:\n",
    "            toreplace = mtch.group(0)\n",
    "            censored = line.replace(toreplace, '*' * len(toreplace))\n",
    "            lines[index] = censored\n",
    "            print(censored)\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "censored = censor(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions in Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `grep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -R | grep '\\.md'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -R | grep '\\.md' | sed 's|^.*\\.md$|newname.md|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!for f in $(ls -R); do newname=$(echo \"$f\" | sed 's|\\.md|-markdown.md|'); echo \"$newname\"; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources around the web\n",
    "\n",
    "### Checkers\n",
    "\n",
    "* [regexr](http://regexr.com/)\n",
    "* [regextester](http://www.regextester.com/)\n",
    "\n",
    "### Cheatsheets\n",
    "\n",
    "* [https://www.debuggex.com/cheatsheet/regex/python](https://www.debuggex.com/cheatsheet/regex/python)\n",
    "* [pyregex](http://www.pyregex.com/)\n",
    "\n",
    "### Crosswords\n",
    "\n",
    "* [regexcrossword](https://regexcrossword.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
